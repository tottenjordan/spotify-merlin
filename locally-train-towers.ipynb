{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb73f72-9b6a-4416-929e-a93223b0728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gcsfs gsutil tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80fa7faf-fcf5-4abf-b54a-ad3c7201da62",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nvtabular'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30234/3539870481.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnvtabular\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnvt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# disable INFO and DEBUG logging everywhere\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nvtabular'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nvtabular as nvt\n",
    "from time import time\n",
    "\n",
    "# disable INFO and DEBUG logging everywhere\n",
    "import logging\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "from nvtabular.ops import (\n",
    "    Categorify,\n",
    "    TagAsUserID,\n",
    "    TagAsItemID,\n",
    "    TagAsItemFeatures,\n",
    "    TagAsUserFeatures,\n",
    "    AddMetadata,\n",
    "    ListSlice\n",
    ")\n",
    "import nvtabular.ops as ops\n",
    "\n",
    "from merlin.schema.tags import Tags\n",
    "\n",
    "import merlin.models.tf as mm\n",
    "from merlin.io.dataset import Dataset\n",
    "import tensorflow as tf\n",
    "\n",
    "# for running this example on CPU, comment out the line below\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13af5f1-13bc-4461-9211-2d1b8620b7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = 'gs://spotify-builtin-2t'\n",
    "PROJECT = 'hybrid-vertex'\n",
    "\n",
    "LOCATION = 'us-central1'\n",
    "\n",
    "train = nvt.Dataset(f\"{BUCKET}/train_data_parquet/0000000000**.snappy.parquet\")\n",
    "valid = nvt.Dataset(f\"{BUCKET}/validation_data_parquet/00000000000*.snappy.parquet\")\n",
    "MAX_PADDING = 375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee0c5a9-cb66-4904-84f6-fdcc1c1a1d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id = [\"track_uri_can\"] >> Categorify(dtype=\"int32\") >> ops.TagAsItemID() >> ops.AddMetadata(tags=[\"user_item\"])\n",
    "# playlist_id = [\"pid_pos_id\"] >> Categorify(dtype=\"int32\") >> TagAsUserID() \n",
    "\n",
    "\n",
    "item_features_cat = ['artist_name_can',\n",
    "        'track_name_can',\n",
    "        'artist_genres_can',\n",
    "    ]\n",
    "\n",
    "item_features_cont = [\n",
    "        'duration_ms_can',\n",
    "        'track_pop_can',\n",
    "        'artist_pop_can',\n",
    "        'artist_followers_can',\n",
    "    ]\n",
    "\n",
    "playlist_features_cat = [\n",
    "        'artist_name_seed_track',\n",
    "        'artist_uri_seed_track',\n",
    "        'track_name_seed_track',\n",
    "        'track_uri_seed_track',\n",
    "        'album_name_seed_track',\n",
    "        'album_uri_seed_track',\n",
    "        'artist_genres_seed_track',\n",
    "        'description_pl',\n",
    "        'name',\n",
    "        'collaborative',\n",
    "    ]\n",
    "\n",
    "playlist_features_cont = [\n",
    "        'duration_seed_track',\n",
    "        'track_pop_seed_track',\n",
    "        'artist_pop_seed_track',\n",
    "        'artist_followers_seed_track',\n",
    "        'duration_ms_seed_pl',\n",
    "        'n_songs_pl',\n",
    "        'num_artists_pl',\n",
    "        'num_albums_pl',\n",
    "    ]\n",
    "\n",
    "#subset of features to be tagged\n",
    "seq_feats_cont = [\n",
    "        'duration_ms_songs_pl',\n",
    "        'artist_pop_pl',\n",
    "        'artists_followers_pl',\n",
    "        'track_pop_pl',\n",
    "    ]\n",
    "\n",
    "seq_feats_cat = [\n",
    "        'artist_name_pl',\n",
    "        # 'track_uri_pl',\n",
    "        'track_name_pl',\n",
    "        'album_name_pl',\n",
    "        'artist_genres_pl',\n",
    "        # 'pid_pos_id', \n",
    "        # 'pos_pl'\n",
    "    ]\n",
    "\n",
    "CAT = playlist_features_cat + item_features_cat\n",
    "CONT = item_features_cont + playlist_features_cont\n",
    "\n",
    "item_feature_cat_node = item_features_cat >> nvt.ops.FillMissing()>> Categorify(dtype=\"int32\") >> TagAsItemFeatures()\n",
    "\n",
    "item_feature_cont_node =  item_features_cont >> nvt.ops.FillMissing() >>  nvt.ops.Normalize() >> TagAsItemFeatures()\n",
    "\n",
    "playlist_feature_cat_node = playlist_features_cat >> nvt.ops.FillMissing() >> Categorify(dtype=\"int32\") >> TagAsUserFeatures() \n",
    "\n",
    "playlist_feature_cont_node = playlist_features_cont >> nvt.ops.FillMissing() >>  nvt.ops.Normalize() >> TagAsUserFeatures()\n",
    "\n",
    "playlist_feature_cat_seq_node = seq_feats_cat >> nvt.ops.FillMissing() >> Categorify(dtype=\"int32\") >> ListSlice(MAX_PADDING, pad=True, pad_value=0) >> TagAsUserFeatures() >> nvt.ops.AddTags(Tags.SEQUENCE) \n",
    "\n",
    "playlist_feature_cont_seq_node = seq_feats_cont >> nvt.ops.FillMissing() >>  nvt.ops.Normalize() >> TagAsUserFeatures() >> nvt.ops.AddTags(Tags.SEQUENCE)\n",
    "\n",
    "# define a workflow\n",
    "output = item_id \\\n",
    "+ item_feature_cat_node \\\n",
    "+ item_feature_cont_node \\\n",
    "+ playlist_feature_cat_node \\\n",
    "+ playlist_feature_cont_node \\\n",
    "+ playlist_feature_cont_seq_node \\\n",
    "+ playlist_feature_cat_seq_node \\\n",
    "# playlist_id \\\n",
    "\n",
    "\n",
    "workflow = nvt.Workflow(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f59d666-b734-475a-89fe-9ec78eb47b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6034ca-902b-42a3-94a0-8aec6107e252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Don't truncate text fields in the display\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "valid.to_ddf().head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf364c17-2858-4d81-94d0-52b0675613da",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(BUCKET, \"merlin-processed\")\n",
    "output_train_dir = os.path.join(output_path, 'train/')\n",
    "output_valid_dir = os.path.join(output_path, 'valid/')\n",
    "output_workflow_dir = os.path.join(output_path, 'workflow/')\n",
    "\n",
    "\n",
    "print(f\"Train data dir: {output_train_dir}\\nValid data dir: {output_valid_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de57c1d-947e-4d24-81f6-5e88908cc628",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "time_preproc = 0\n",
    "time_preproc_start = time()\n",
    "\n",
    "workflow.fit_transform(train).to_parquet(output_path=output_train_dir, \n",
    "                                         shuffle=nvt.io.Shuffle.PER_PARTITION,\n",
    "                                         cats=CAT,\n",
    "                                         conts=CONT, \n",
    "                                         output_files = 50\n",
    "                                         ) #preserve_files=True keeps the original file sharding\n",
    "\n",
    "\n",
    "time_preproc += time()-time_preproc_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61840277-9c59-4527-96b7-19cc93a8ee4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the workflow to disk after it has been fit\n",
    "workflow.save(os.path.join(output_workflow_dir,'2t-spotify-workflow'))\n",
    "\n",
    "#locally for demo too\n",
    "workflow.save('2t-spotify-workflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a5d822-80d5-4300-9865-02c603a22a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = workflow.output_schema\n",
    "\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c7305d-1de3-466d-8ac3-5fc13d625583",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "time_preproc_start = time()\n",
    "wf_valid_op = workflow.transform(valid).to_parquet(output_path=output_valid_dir, \n",
    "                                         shuffle=nvt.io.Shuffle.PER_PARTITION,\n",
    "                                         cats=CAT,\n",
    "                                         conts=CONT,\n",
    "                                         output_files=10\n",
    "                                         )\n",
    "\n",
    "\n",
    "time_preproc += time()-time_preproc_start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e58fd7-c92c-4236-bc11-b4443ad07300",
   "metadata": {},
   "source": [
    "# Load the processed data into a Merlin Dataset and inspect the transforms\n",
    "\n",
    "Now that ETL is over, the workflow is saved and data is processed to the `output_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e786d22-4a46-4bc2-b3a3-8f61ea358f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp -r {output_workflow_dir} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c25f31-e593-49c5-a5e5-dc25831e1618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load back the workflow and schema\n",
    "# spotify-builtin-2t/merlin-processed/workflow/2t-spotify-workflow\n",
    "workflow = nvt.Workflow.load(\"2t-spotify-workflow\")\n",
    "schema = workflow.output_schema\n",
    "embeddings = ops.get_embedding_sizes(workflow)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543d2a7b-1523-48bd-804b-d1b05ecaf1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.models.utils.example_utils import workflow_fit_transform\n",
    "\n",
    "from merlin.schema.tags import Tags\n",
    "\n",
    "import merlin.models.tf as mm\n",
    "from merlin.io.dataset import Dataset as MerlinDataset\n",
    "\n",
    "train = MerlinDataset(output_train_dir + \"/*.parquet\", schema=schema, part_size=\"500MB\")\n",
    "valid = MerlinDataset(output_valid_dir + \"/*.parquet\", schema=schema, part_size=\"500MB\")\n",
    "\n",
    "#look at output\n",
    "schema = train.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16df1f47-afac-49e7-adae-54a369c35e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_t_schema = schema.select_by_tag([Tags.ITEM_ID, Tags.ITEM, Tags.USER, Tags.USER_ID])\n",
    "two_t_schema_seq = schema.select_by_tag([Tags.SEQUENCE])\n",
    "non_seq_col_names = list(set(two_t_schema.column_names) - set(two_t_schema_seq.column_names))\n",
    "non_seq_col_names\n",
    "# two_t_schema = [x for x in two_t_schema_seq.column_names]\n",
    "two_t_schema = two_t_schema[non_seq_col_names]\n",
    "two_t_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210b5556-cd25-418e-a2ca-42ef2c056540",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780be8ba-da28-4993-8a5a-6e84bee917c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format embeddings\n",
    "embeddings_all = embeddings\n",
    "\n",
    "emb_dims = {}\n",
    "for k in list(embeddings_all.keys()):\n",
    "    emb_dims.update({k: embeddings_all[k][1]})\n",
    "emb_dims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae6f7ba-d758-4440-bb4e-c2a01e8297d0",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bcb50c-d44b-4c9a-949c-32e38f9a0ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mm.TwoTowerModel(\n",
    "    two_t_schema,\n",
    "    query_tower=mm.MLPBlock([1024,512,256], no_activation_last_layer=True),\n",
    "    item_tower=mm.MLPBlock([1024,512,256], no_activation_last_layer=True),\n",
    "    samplers=[mm.InBatchSampler()],\n",
    "    embedding_options=mm.EmbeddingOptions(infer_embedding_sizes=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98b2593-62c2-4bd8-a3b0-8683b8425d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model.compile(optimizer=\"adam\", run_eagerly=False, metrics=[mm.RecallAt(1), mm.RecallAt(10), mm.NDCGAt(10)])\n",
    "model.fit(train, validation_data=valid, batch_size=2048, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cf8cef-b085-44e2-b55a-9fe22aea21c6",
   "metadata": {},
   "source": [
    "### Save Query Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2052be-072d-42a1-8132-8c2eccb101df",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_path = os.path.join(output_path, 'spotify-2t-query-model')\n",
    "model.save(artifact_path) #saves keras model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdc9287-f4a3-4603-8aed-e1e61a8915b0",
   "metadata": {},
   "source": [
    "### Save Track Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e401c16-be47-4e71-9f03-faa41e6802d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.models.utils.dataset import unique_rows_by_features\n",
    "\n",
    "item_features = (\n",
    "    unique_rows_by_features(train, Tags.ITEM, Tags.ITEM_ID)\n",
    "    .compute()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "item_embs = model.item_embeddings(\n",
    "    MerlinDataset(item_features, schema=schema), batch_size=1024\n",
    ")\n",
    "item_embs_df = item_embs.compute(scheduler=\"synchronous\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-9.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-9:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
